{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned Based Models for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Embeddings & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = joblib.load('data/train_embeddings.pkl')\n",
    "y_train = pd.read_csv('data/training.csv')['label'].values\n",
    "\n",
    "X_val = joblib.load('data/validation_embeddings.pkl')\n",
    "y_val = pd.read_csv('data/validation.csv')['label'].values\n",
    "\n",
    "X_test = joblib.load('data/test_embeddings.pkl')\n",
    "y_test = pd.read_csv('data/test.csv')['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 768) (16000,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(type(X_train), type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dataframe into Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset): # defines how the text is pre-processed before sending it to the neural network.\n",
    "    # generates \n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings  # Lista de embeddings generados por RoBERTa\n",
    "        self.labels = labels.squeeze()  # Lista de etiquetas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): # []\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.EmotionDataset'> <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "# Dataset type conversion\n",
    "train_dataset = EmotionDataset(X_train, y_train)\n",
    "val_dataset = EmotionDataset(X_val, y_val)\n",
    "test_dataset = EmotionDataset(X_test, y_test)\n",
    "\n",
    "# will feed the data in batches to the neural network for suitable training and processing.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(type(train_dataset), type(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model\n",
    "\n",
    "Parameters:\n",
    "- Input_size: Embedding length\n",
    "- Heads: Number of heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)  # Adjust input size if needed\n",
    "    \n",
    "    def forward(self, embeddings):  # Only embeddings are required\n",
    "        output_2 = self.l2(embeddings)\n",
    "        output = self.l3(output_2)\n",
    "        # output = torch.argmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for _, data in enumerate(train_loader, 0):\n",
    "        # Unpack the data from the DataLoader\n",
    "        embeddings, targets = data\n",
    "        embeddings = embeddings.to(device, dtype=torch.float32)  # Move embeddings to the device\n",
    "        targets = targets.to(device, dtype=torch.long)  # Move targets to the device\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(embeddings)\n",
    "\n",
    "        # Calculate loss\n",
    "        optimizer.zero_grad()\n",
    "        # One-hot encoding\n",
    "        one_hot = torch.nn.functional.one_hot(targets, num_classes=6).float() # loss function needs it like this, idk\n",
    "        loss = loss_fn(outputs, one_hot)\n",
    "        loss_history.append(loss)\n",
    "        if _ % 5000 == 0:  # Print loss for every 5000 steps\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.989832878112793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.469208002090454\n",
      "Epoch: 2, Loss: 1.3130199909210205\n",
      "Epoch: 3, Loss: 1.2169042825698853\n",
      "Epoch: 4, Loss: 1.1697132587432861\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(test_loader, 0):\n",
    "            embeddings, targets = data\n",
    "            embeddings = embeddings.to(device, dtype=torch.float32)  # Move embeddings to the device\n",
    "            targets = targets.to(device, dtype=torch.long)  # Move targets to the device\n",
    "            outputs = model(embeddings)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.5995\n",
      "F1 Score (Micro) = 0.5995\n",
      "F1 Score (Macro) = 0.5137334326669131\n",
      "Accuracy Score = 0.5995\n",
      "F1 Score (Micro) = 0.5995\n",
      "F1 Score (Macro) = 0.5137334326669131\n",
      "Accuracy Score = 0.5995\n",
      "F1 Score (Micro) = 0.5995\n",
      "F1 Score (Macro) = 0.5137334326669131\n",
      "Accuracy Score = 0.5995\n",
      "F1 Score (Micro) = 0.5995\n",
      "F1 Score (Macro) = 0.5137334326669131\n",
      "Accuracy Score = 0.5995\n",
      "F1 Score (Micro) = 0.5995\n",
      "F1 Score (Macro) = 0.5137334326669131\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    results = np.array([np.argmax(inner) for inner in outputs])\n",
    "    accuracy = accuracy_score(targets, results)\n",
    "    f1_score_micro = f1_score(targets, results, average='micro')\n",
    "    f1_score_macro = f1_score(targets, results, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick base pre-trained model\n",
    "\n",
    "# define feature extraction layer(docking layer to classification head), usually \"bottle-neck layer\"(last layer before flatten op)\n",
    "\n",
    "#Feature extraction\n",
    "#In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor\n",
    "# #Additionally, you add a classifier on top of it and train the top-level classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classification head (class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training & Validation Loss\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Training & Validation F1-score\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# training & Validation Loss\n",
    "# Training & Validation F1-score\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be done if we finish the bert model on time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
